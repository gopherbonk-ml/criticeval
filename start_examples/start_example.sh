#!/usr/bin/env bash
export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export HYDRA_FULL_ERROR=1
export VLLM_LOGGING_LEVEL=DEBUG
export ASCEND_LAUNCH_BLOCKING=1


python -m criticeval.eval \
  evaluation.mode="all" \
  paths.data_dir="data" \
  paths.save_dir="outputs" \
  data.problem_file="problems_example.csv" \
  outputs.experiment_id="example_run" \
  outputs.save_solver_outputs=True \
  template.solver_templates=["base_solver"] \
  template.judger_templates=["base_judger"] \
  template.use_extract_answer_for_solver=True \
  template.use_extract_answer_for_judger=True \
  template.extract_answer_func_for_solver="boxed_answer_extractor" \
  template.extract_answer_func_for_judger="boxed_answer_extractor" \
  solver.backend.backend_module="vllm" \
  solver.backend.vllm.model="/workspace/local/models/Qwen2.5-VL-3B-Instruct" \
  solver.backend.vllm.dtype="auto" \
  solver.backend.vllm.device="auto" \
  solver.backend.vllm.num_devices=1 \
  solver.backend.vllm.tensor_parallel_size=2 \
  solver.backend.vllm.max_model_length=8192 \
  solver.backend.vllm.gpu_memory_utilization=0.6 \
  solver.backend.vllm.trust_remote_code=True \
  solver.backend.vllm.enforce_eager=True \
  solver.backend.vllm.num_devices=1 \
  solver.sampling_params.temperature=0 \
  solver.sampling_params.max_tokens=4096 \
  solver.sampling_params.top_p=0.9 \
  judger.backend.backend_module="vllm" \
  judger.backend.vllm.device="auto" \
  judger.backend.vllm.num_devices=1 \
  judger.backend.vllm.model="/workspace/local/models/Qwen2.5-VL-3B-Instruct" \
  judger.backend.vllm.dtype="auto" \
  judger.backend.vllm.device="npu" \
  judger.backend.vllm.tensor_parallel_size=4 \
  judger.backend.vllm.max_model_length=8192 \
  judger.backend.vllm.gpu_memory_utilization=0.6 \
  judger.backend.vllm.trust_remote_code=True \
  judger.backend.vllm.enforce_eager=True \
  judger.sampling_params.temperature=0.6 \
  judger.sampling_params.max_tokens=4096 \
  judger.sampling_params.top_p=0.9 \
